{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "5ad08ec8-783b-4fd5-86a0-b269b5e0118d",
    "deletable": false
   },
   "source": [
    "# Homework 3 \u2014 Neural models\n",
    "\n",
    "In this homework, we will use *neural networks* as models for the IOB task that we worked on in the last assignment. In these models, the inference methods that worked well for FSTs are no longer usable, so we will need new decision agents. Equipped with these new tools, we will end the assignment in the construction of a transition-based neural dependency parser.\n",
    "\n",
    "For now, let's start with the usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "005ead6f-ac5a-4681-8c5e-d8d16473409f",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "num_threads = 1  # if you change this number you have to restart the notebook\n",
    "\n",
    "# A hyperparameter for the size of all the hidden layers throughout the notebook.\n",
    "# Note that it is uncommon to just use one number for everything and it can in fact\n",
    "# complicate debugging -- but we do it for simplicity in this homework.\n",
    "# Bonus pro tip: use prime numbers while prototyping, you can always see where your\n",
    "# product come from!\n",
    "HIDDEN_SIZE = 30\n",
    "\n",
    "# set this before we import numpy\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = str(num_threads)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "import sys\n",
    "import re\n",
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "import math\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, Tensor, FloatTensor, LongTensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_num_threads(num_threads)\n",
    "\n",
    "\n",
    "from seq2class_homework1 import (\n",
    "    TaskSetting,\n",
    "    ProbabilityModel,\n",
    "    BoltzmannModel,\n",
    "    DecisionAgent,\n",
    "    ViterbiAgent,\n",
    "    BayesAgent,\n",
    "    L2LogLikelihood,\n",
    "    SGDTrainer\n",
    ")\n",
    "\n",
    "from seq2class_homework2 import (\n",
    "    IobTask0,\n",
    "    Integerizer,\n",
    "    F1\n",
    ")\n",
    "\n",
    "Data_type = namedtuple('Data', ['xx', 'oo', 'yy'])\n",
    "\n",
    "# data loading for the IOB task\n",
    "def iterate_data(filename='train', *, max_examples=None):\n",
    "    file = open(f'iob/{filename}.tsv')\n",
    "    for n, row in enumerate(csv.DictReader(file, delimiter='\\t')):\n",
    "        if max_examples and n >= max_examples:\n",
    "            break\n",
    "        yield Data_type(\n",
    "            xx=tuple(row['xx'].split()),\n",
    "            oo=None,  # we are not dealing with partial observations in this homework\n",
    "            yy=tuple(row['yy'].split()) if 'yy' in row else None,\n",
    "        )\n",
    "\n",
    "        \n",
    "# data loading for the parsing task\n",
    "Parse_Data_type = namedtuple('ParseData', ['xx', 'tree'])\n",
    "def iterate_trees(filename='train', *, max_examples=None):\n",
    "    file = open(f'trees/{filename}.tsv')\n",
    "    for n, row in enumerate(csv.DictReader(file, delimiter='\\t')):\n",
    "        if max_examples and n >= max_examples:\n",
    "            break\n",
    "        yield Parse_Data_type(\n",
    "            xx=tuple(row['xx'].split(' ')),\n",
    "            tree=tuple(map(int, row['tree'].split())),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "ec8d53c9-cc80-4dc9-a085-de89ffb77d18",
    "deletable": false
   },
   "source": [
    "## PyTorch Basics\n",
    "\n",
    "For this assignment we will be using [PyTorch](http://pytorch.org/) to build a neural transition-based parser.  We will build up to writing a full parser by first constructing a neural representation of our words from the character sequence of each word.  We will then use this character representation to train a neural IOB tagger and eventually our neural parser.\n",
    "\n",
    "Here are some useful pointers to the PyTorch docs:\n",
    "\n",
    "#### Documentation links:\n",
    "1. [Torch general documentation](http://pytorch.org/docs/stable/torch.html)\n",
    "2. [Torch neural network layers](http://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "#### Neural network layers that we will use through this assignment\n",
    "| Neural Layer | Use |\n",
    "|---|---|\n",
    "| [nn.Linear](http://pytorch.org/docs/master/nn.html#torch.nn.Linear) | A single feed forward linear layer with bias. |\n",
    "| [nn.LSTMCell](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell) | Implements a single step of an LSTM returning the new hidden and cell states. |\n",
    "| [nn.Embedding](http://pytorch.org/docs/master/nn.html#torch.nn.Embedding) | Holds an embedding matrix for tokens indexed by an integer array. |\n",
    "| [nn.Parameter](http://pytorch.org/docs/master/nn.html#torch.nn.Parameter) | Holds a raw tensor but adds it to the list of parameters for training |\n",
    "\n",
    "#### Useful PyTorch functions :\n",
    "| Function | Use |\n",
    "|---|---|\n",
    "| [torch.cat](http://pytorch.org/docs/stable/torch.html#torch.cat) | concatenate tensors along a given dimension |\n",
    "| [torch.stack](http://pytorch.org/docs/stable/torch.html#torch.stack) | adds a new dimension when combing tensors |\n",
    "| [torch.tanh](http://pytorch.org/docs/stable/torch.html#torch.tanh) | Hyperbolic tangent function (non-linearity) |\n",
    "| [torch.softmax](http://pytorch.org/docs/stable/torch.html#torch.softmax) | Compute the softmax over a tensor |\n",
    "| [torch.zeros](http://pytorch.org/docs/stable/torch.html#torch.zeros) | Returns a new tensor filled with zeros |\n",
    "| [torch.rand](http://pytorch.org/docs/stable/torch.html#torch.rand) | Returns a new tensor filled with random values in the $[0, 1)$ interval |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "0ba5aa14-6ceb-43d2-9931-d4c0efed25a9",
    "deletable": false
   },
   "source": [
    "# Part 1: Backpropagating through Matrices and Checking your Answer\n",
    "\n",
    "Backward-mode automatic differentiation a.k.a. backpropagation is the algorithmic workhorse of \n",
    "deep learning. It fuses the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), taken from calculus, and [memoization](https://en.wikipedia.org/wiki/Memoization), a technique from algorithms. \n",
    "\n",
    "Let\u2019s first talk about the calculus. As students, you likely spent a lot of time manually taking derivatives with respect to real scalars in univariate calculus and with respect to real vectors in multi-variate calculus. If you took complex analysis, you may have even taken derivatives with respect to complex-valued quantities. But what of matrices? In deep learning, we are often interested in the derivative with respect to a real matrix. For instance, consider this simple log-linear model for binary classification: $p(y \\mid x) = \\textit{sigmoid}(W x)$, where $x \\in \\mathbb{R}^d$ is the input to the model, $W \\in \\mathbb{R}^{2 \\times d}$ is a matrix of parameters. Typically, we choose $W$ so as to minimize the loss ${\\cal L}(W) = -\\sum_{i=1}^n \\log p(y^{(i)} \\mid x^{(i)})$, perhaps with a regularization terms, which we would achieve algorithmically by following the gradient $\\frac{\\partial L}{\\partial W}$ until we reach a (local) optimum. \n",
    "\n",
    "To get you comfortable with taking derivatives with respect to matrices, let's go through a few theoretical exercises to get you warmed up. Note that the general strategy will be to break down the function into a series of sums, using the definition of the matrix operations, and, then, apply standard techniques from univariate calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "bc163f50-0695-4698-8518-190dd04e34c2",
    "deletable": false
   },
   "source": [
    "### Question 1.1: The Derivative of Latent Semantic Analysis (LSA)\n",
    "\n",
    "Suppose you have a large matrix $X \\in \\mathbb{R}^{N \\times M}$ and you would like to approximately factorize it.  This *matrix factorization* problem pops up in *latent semantic analysis* (LSA, an old pre-neural technique to get word embeddings), where $X_{ij}$ is the number of times word $i$ appears in context $j$.  In this question, however, we will generalize by allowing the entries of $X$ to be real numbers rather than counts.\n",
    "\n",
    "Let us try to express $X \\approx AB$ where $A \\in \\mathbb{R}^{N \\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$.  Note that $\\textit{rank}(AB) \\leq K$. By setting $K$ to a small value, we can ensure that this rank is less than the original $\\textit{rank}(X)$, which may be as large as $\\min(N,M)$.  Thus, we regard $AB$ as a low-rank approximation to $X$.\n",
    "\n",
    "Specifically, we will minimize ${\\cal L}_\\textit{lsa}(A, B) = \\frac{1}{2}||X - AB||^2_F$, where $||\\cdot||_F$ denotes the [Frobenius norm](http://mathworld.wolfram.com/FrobeniusNorm.html) of a matrix.\n",
    "\n",
    "While we can globally minimize this particular objective with use of the Singular Value Decomposition, one may also use gradient descent, albeit without the globally optimal guarantee. So, for practice, let's figure out the gradient of the objective with respect to the parameters\u2014namely the elements of $A$ and $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "2bf433f7-ed77-4b2b-93b2-88e1a1945d8a",
    "deletable": false
   },
   "source": [
    "#### The Gradient of Matrix Multiplication\n",
    "\n",
    "More specifically, the goal is to compute $\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial A}$ and $\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial B}$. But what does that even mean when $A$ and $B$ are matrices?  We'd better try temporarily eliminating the matrix notation and just using good old single-variable calculus:\n",
    "$$ {\\cal L}_\\textit{lsa}(A, B) = \\frac{1}{2}||X - AB||^2_F = \\frac{1}{2}\\sum_{i'=1}^N \\sum_{j'=1}^M (X_{i'j'} - \\sum_{k'=1}^K A_{i'k'} B_{k'j'})^2$$\n",
    "\n",
    "Now for any fixed $i,j,k$, you should be able to write down formulas for both $\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial A_{ik}}$ and $\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial B_{kj}}$.  (We used $i',j',k'$ in the objective function above so that you could keep the summation indices separate from the indices of the specific fixed variable that you are differentiating with respect to.)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial A_{ik}} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial B_{kj}} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Congratulations: you have just written formulas for computing the $NK + KM$ different partial derivatives.  It's convenient to store all those real numbers in a pair of matrices, which are what we call $\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial A} \\in \\mathbb{R}^{N \\times K}$ and $\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial B} \\in \\mathbb{R}^{K \\times M}$.  \n",
    "\n",
    "Can you write formulas that compute those matrices using only matrix operations (which are fast in numpy and PyTorch)?  Check: the correct answer $\\textit{only}$ requires multiplication, subtraction, and transposition on matrices.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial A} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\frac{\\partial {\\cal L}_\\textit{lsa}}{\\partial B} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "f4da44cd-8ed4-4ef5-a987-2a92955a9054",
    "deletable": false
   },
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Recall that PyTorch will automatically compute gradients for you using [reverse-mode automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation). This algorithm was discussed and demoed in class. See it in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "2cd95e7f-bbf5-4495-b9f6-d16acc71349d",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "n = 5; k = 2\n",
    "# random initialization of the parameters\n",
    "A = torch.randn(n, k, requires_grad=True)\n",
    "B = torch.randn(k, n, requires_grad=True)\n",
    "# random positive matrix to factorize\n",
    "X = torch.exp(torch.randn(n, n, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "93811aca-c22c-4ee3-b367-a056701953cb",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def Llsa(A, B):\n",
    "    \"\"\"\n",
    "    Loss function for matrix factorization\n",
    "    \"\"\"\n",
    "    C = torch.mm(A, B)\n",
    "    loss = 0.5 * ((X-C)**2).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "73c2bc41-ac7c-4b90-abce-95c8772ed8c3",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "A.grad = None\n",
    "B.grad = None\n",
    "loss = Llsa(A, B)\n",
    "loss.backward()\n",
    "dLlsa_dA_auto = A.grad.numpy()\n",
    "dLlsa_dB_auto = B.grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "e83b700a-fdef-4f7e-93cc-a995521212be",
    "deletable": false
   },
   "source": [
    "### Now, fill in the following functions with your answer from above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "70912acb-fa81-4165-8680-9a5e206dea68",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def dLlsa_dA(A, B):\n",
    "    \"\"\" Computes the derivative of L with respect to the matrix A\"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "f3e69886-1c60-45b8-a888-24c14a621ac8",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def dLlsa_dB(A, B):\n",
    "    \"\"\" Computes thederivative of L with respect to the matrix B \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "af9b04a2-cd40-42f8-88fe-27bb6f51d533",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# the following two assert statements should pass if you did it right :-)\n",
    "assert np.allclose(dLlsa_dA_auto, (dLlsa_dA(A, B)), atol=1e-2)\n",
    "assert np.allclose(dLlsa_dB_auto, (dLlsa_dB(A, B)), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "0b0c5d95-f756-460f-b703-81f88d5e192e",
    "deletable": false
   },
   "source": [
    "### Question 1.2: The Derivative of Skip-Gram\n",
    "\n",
    "Again, let $X \\in \\mathbb{R}^{N \\times M}$, $A \\in \\mathbb{R}^{N \\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$ be real-valued matrices. We will\n",
    "consider the objective function \n",
    "\n",
    "${\\cal L}_{\\textit{skipgram}}(A, B) = \\sum_{i=1}^N \\sum_{j=1}^M X_{ij} \\log p(i \\mid j ; A, B)$, \n",
    "\n",
    "whose derivative you will find with respect to both $A$ and $B$. We define the following \n",
    "\n",
    "$\\log p(i \\mid j; A, B) = \\log \\textit{softmax}((AB)_{:j})_{i} = \\log \\frac{\\exp\\,(AB)_{ij}}{\\sum_{i'=1}^N \\exp\\, (AB)_{i'j}} = (AB)_{ij} - \\log \\sum_{i'=1}^N \\exp\\, (AB)_{i'j}$\n",
    "\n",
    "Recall from class that this objective is called the [skip-gram](https://en.wikipedia.org/wiki/Word2vec) objective, which is what word2vec is trying to optimize for.\n",
    "Again, the basic strategy for finding the derivatives is the same---break the objective down into summands. Compute:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\cal L}_\\textit{skipgram}}{\\partial A} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\frac{\\partial {\\cal L}_\\textit{skipgram}}{\\partial B} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "0c352186-3286-4eab-8c5c-b249fbfa21a6",
    "deletable": false
   },
   "source": [
    "Let's implement this again (we'll just reuse the same matrices, thus setting $M=N$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "be8620c1-96f8-4ac6-be05-709a506b6e7c",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def Lskipgram(A, B):\n",
    "    logit = torch.mm(A, B)\n",
    "    return (X * F.log_softmax(logit, dim=0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "970d24c7-1bc1-4667-bf65-256d458d8f99",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Renew the computation graph -- by simply starting from scratch.\n",
    "A = torch.randn(n, k, requires_grad=True)\n",
    "B = torch.randn(k, n, requires_grad=True)\n",
    "X = torch.exp(torch.randn(n, n, requires_grad=True))\n",
    "# HINT: these two matrices (and their names) should help you see relation\n",
    "# between the gradient of skip-gram's matrices and those of the log-linear model\n",
    "# you saw in NLP assignment 3. \n",
    "logit = torch.mm(A, B)\n",
    "P = F.softmax(logit, dim=0)\n",
    "X_expected = torch.mm(P, torch.diag(X.sum(0).squeeze(0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "27906fbb-b161-4795-ae44-e8a556d4830a",
    "deletable": false
   },
   "source": [
    "Now, fill in the following functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "be3cdb30-8a05-4ea1-87df-40b83f974971",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def dLskipgram_dA(A, B):\n",
    "    \"\"\" Computes the derivative of L with respect to the matrix A\"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "dffa13f5-8509-4fb4-806b-f3da93ca6773",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def dLskipgram_dB(A, B):\n",
    "    \"\"\" Computes thederivative of L with respect to the matrix B \"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "7eb353ea-b7f9-45c5-8936-4762fa55932e",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# automatic differentiation again\n",
    "loss = Lskipgram(A, B)\n",
    "loss.backward()\n",
    "dLskipgram_dA_auto = A.grad.data.numpy()\n",
    "dLskipgram_dB_auto = B.grad.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "162cc655-bd5f-448b-a196-d8da14c67c78",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# the following two assert statements should pass if you did it right :-)\n",
    "assert np.allclose(dLskipgram_dA_auto, (dLskipgram_dA(A, B)), atol=1e-2)\n",
    "assert np.allclose(dLskipgram_dB_auto, (dLskipgram_dB(A, B)), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "8e810ffb-934f-4e17-8511-9e3164d08978",
    "deletable": false
   },
   "source": [
    "### Question 1.3: The finite-difference check\n",
    "\n",
    "Let\u2019s suppose, for a second, we didn\u2019t have an automatic-differentiation toolkit available. Is there, perhaps, another way that we can use a computer to compute the gradient? The most common numerical method at our disposal is called the finite-difference check.\n",
    "\n",
    "#### Background\n",
    "Consider a smooth function $f : \\mathbb{R}\\rightarrow \\mathbb{R}$. Recall from calculus,\n",
    "that for $x, \\varepsilon \\in \\mathbb{R}$, the first-order Taylor approximation of $f$ may be written\n",
    "as\n",
    "\n",
    "$\n",
    "f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + o(|\\varepsilon|^2)\n",
    "$\n",
    "\n",
    "If little-o notation is unfamiliar to you, just interpret $o(|\\varepsilon|^2)$\n",
    "as the order of magnitude of the error term. More often, this formula is written as,\n",
    "\n",
    "$\n",
    "    f'(x) \\approx  \\frac{f(x + \\varepsilon) - f(x)}{\\varepsilon}\n",
    "$\n",
    "\n",
    "The equation above is termed the **forward difference** in the numerical analysis literature.  \n",
    "Note that in the limit as $\\varepsilon \\rightarrow 0$, the equation is exact. Indeed,\n",
    "you may recognize this as the definition of the derivative. We may also consider the\n",
    "**backward difference**\n",
    "\n",
    "$\n",
    "    f'(x) \\approx  \\frac{f(x) - f(x - \\varepsilon)}{\\varepsilon}\n",
    "$\n",
    "\n",
    "which follows were we to replace $(x+\\varepsilon)$ with $(x-\\varepsilon)$ in the derivation\n",
    "of the forward difference from the Taylor approximation. Averaging the forward and backward difference,\n",
    "yields the **central difference**.\n",
    "\n",
    "$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2\\varepsilon}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "bdbb8c02-b076-40e4-9250-de33712a64ed",
    "deletable": false
   },
   "source": [
    "### Operationalizing the finite-difference check\n",
    "\n",
    "Now, let's operationalize the finite-difference check. Given a function $f(x) = \\sin(x^2) + 10$, we are going to ask you to implement the finite-difference method and compare the output to a derivative you compute analytically: *checking* the correctness of derivative and gradient computations is the primary use case of the finite-difference method, after all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "701b0b5b-572d-4acc-8870-9666702590a1",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" a fun function\"\"\"\n",
    "    return np.sin(x**2) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "6e6128ea-deae-42ab-9183-30196045fb70",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    \"\"\" compute the derivative of f symbolically\"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "70a868c9-5536-4ec6-afef-fa039f57c0a6",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def df_fd(x, eps=1e-5):\n",
    "    \"\"\" compute the derivative of f using the finite-difference check\"\"\"\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "24c765df-05e7-4559-a36e-de017297a907",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# if you have done it right, these two checks should pass\n",
    "assert np.allclose(df(5.0), df_fd(5.0), atol=1e-2)\n",
    "assert np.allclose(df(-2.0), df_fd(-2.0), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "89c653a3-0d08-4720-9efd-ad4b25edc249",
    "deletable": false
   },
   "source": [
    "### Question 1.4: Extension to functions of many variables\n",
    "\n",
    "In the land of machine learning, we are generally interested in computing the partial derivatives of a function\n",
    "of many variables: $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$. (The vector of partial derivatives is called the gradient.) How do we extend the finite-difference check to this case? As it turns out, it's quite trivial. Say we have a function $f(x_1, \\ldots, x_n)$: to compute $\\nabla_{\\mathbf{x}} f$, we simply perform the univariate finite-difference check on each component of the input individually. Write a function to do this for each of the parameters $A \\in \\mathbb{R}^{N \\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "d6bc97c5-43e8-4b87-adb8-5aebabc60d32",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def finite_difference(Y, f, eps=1e-2):\n",
    "    \"\"\" compute the finite-difference approximation for the function f\"\"\"\n",
    "    # populate this numpy matrix\n",
    "    dY = np.zeros((Y.size(0), Y.size(1)))\n",
    "    ### STUDENTS START\n",
    "    raise NotImplementedError()  # REPLACE ME\n",
    "    ### STUDENTS END\n",
    "    return dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "acb278df-e849-45fa-8523-212ecf699e7e",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# the following two assert statements should pass if you did it right :-)\n",
    "assert np.allclose(dLskipgram_dA_auto, finite_difference(A, lambda Y: Lskipgram(Y, B)), atol=1e-2)\n",
    "assert np.allclose(dLskipgram_dB_auto, finite_difference(B, lambda Y: Lskipgram(A, Y)), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "89c653a3-0d08-4720-9efd-ad4b25edc249",
    "deletable": false
   },
   "source": [
    "### Question 1.5: Runtime of the Finite-Difference Check\n",
    "\n",
    "Finally, let's analyze the runtime of the finite-difference check and compare it to the runtime of automatic differentiation.\n",
    "\n",
    "Let $f\\colon \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a differentiable function that can be computed in $\\textrm{runtime}(f)$. In big-$O$ notation, we would write that we can compute $f$ in $O(\\textrm{runtime}(f))$. \n",
    "\n",
    "As a function of $\\textrm{runtime}(f)$ and $n$, give (in big-$O$ notation) the following two runtimes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{runtime of backward-mode automatic differentiation for all $n$ parameters}  &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\text{runtime of the finite-difference check for all $n$ parameters} &= \\color{red}{\\text{FILL IN}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "What does this analysis ever tell you about the practicality of the finite-difference method for computing the derivatives within an optimization routine for neural networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "d2261a0d-687f-46e1-8b1d-d545c4cb9988",
    "deletable": false
   },
   "source": [
    "# Part 2: Embedding words using their character representation\n",
    "\n",
    "As a warm-up, we will embed actual words using recurrent neural networks, specifically using long short-term memory (LSTM). We would like for each word to be represented as a single fixed sized vector, this will allow us to more easily feed into later stages of our training pipe line.  We are going to accomplish this using a character level LSTM over the characters in the word.\n",
    "\n",
    "For each word in the sequence `xx`, we are going to split it into characters and represent each character an integer.  To pass this representation into the LSTM, we will use a one-hot encoding, meaning at each step we will apply the LSTM to a tensor that contains a single 1 value and zeros otherwise.\n",
    "\n",
    "\n",
    "### PyTorch Basics\n",
    "\n",
    "For this assignment we will be using [PyTorch](http://pytorch.org/) to build a neural transition-based parser.  We will build up to writing a full parser by first constructing a neural representation of our words from the character sequence of each word.  We will then use this character representation to train a neural IOB tagger and eventually our neural parser.\n",
    "\n",
    "Here are some useful pointers to the PyTorch docs:\n",
    "\n",
    "#### Documentation links:\n",
    "1. [Torch general documentation](http://pytorch.org/docs/stable/torch.html)\n",
    "2. [Torch neural network layers](http://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "#### Neural network layers that we will use through this assignment\n",
    "| Neural Layer | Use |\n",
    "|---|---|\n",
    "| [nn.Linear](http://pytorch.org/docs/master/nn.html#torch.nn.Linear) | A single feed forward linear layer with bias. |\n",
    "| [nn.LSTMCell](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell) | Implements a single step of an LSTM returning the new hidden and cell states. |\n",
    "| [nn.Embedding](http://pytorch.org/docs/master/nn.html#torch.nn.Embedding) | Holds an embedding matrix for tokens indexed by an integer array. |\n",
    "| [nn.Parameter](http://pytorch.org/docs/master/nn.html#torch.nn.Parameter) | Holds a raw tensor but adds it to the list of parameters for training |\n",
    "\n",
    "#### Useful PyTorch functions :\n",
    "| Function | Use |\n",
    "|---|---|\n",
    "| [torch.cat](http://pytorch.org/docs/stable/torch.html#torch.cat) | concatenate tensors along a given dimension |\n",
    "| [torch.stack](http://pytorch.org/docs/stable/torch.html#torch.stack) | adds a new dimension when combing tensors |\n",
    "| [torch.tanh](http://pytorch.org/docs/stable/torch.html#torch.tanh) | Hyperbolic tangent function (non-linearity) |\n",
    "| [torch.softmax](http://pytorch.org/docs/stable/torch.html#torch.softmax) | Compute the softmax over a tensor |\n",
    "| [torch.zeros](http://pytorch.org/docs/stable/torch.html#torch.zeros) | Returns a new tensor filled with zeros |\n",
    "| [torch.rand](http://pytorch.org/docs/stable/torch.html#torch.rand) | Returns a new tensor filled with random values in the $[0, 1)$ interval |\n",
    "\n",
    "You should probably also familiarize yourself with different ways to inspect tensors, querying their `shape`, `size()`, `dim`, and the (still fairly new) zero-dimensional tensors, who encapsulate single scalars, acessible via `item()`.\n",
    "\n",
    "### A simple implementation\n",
    "\n",
    "Look at the code below.\n",
    "\n",
    "In PyTorch, neural modules are often encapsulated into classes that inherit from `nn.Module` -- this gives them useful features like keeping track of all the parameters that we want to learn.\n",
    "\n",
    "More specifically, see how the `CharacterLSTMPreprocessModule` has two *parameters*: an `nn.LSTMCell` and an initial vector.\n",
    "PyTorch modules are executed by calling the `forward()` method.\n",
    "Here, we loop over the characters in the input word, encode each as a [one-hot vector](https://en.wikipedia.org/wiki/One-hot), and, then, calls ``self.character_lstm``. In short, you are unfolding the LSTM over the character stream. The sanity checks below should help guide you. You will implement similar methods in the next questions.\n",
    "\n",
    "You will notice that we have a first dimension of size 1 on both the one-hot vector and the hidden states. This dimension is usually used for *batching*, i.e., processing multiple data points at the same time. We will for now limit ourselves to just a single data point, so this dimension is of size 1 (omitting it makes some PyTorch methods fail).\n",
    "\n",
    "(If you are interested, spend a few minutes thinking about possible complications here! Batching variable-length sequences is a little tricky and all frameworks have come up with different solutions for them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "60fe60cb-9e3c-4e5d-a0f3-38508b781fda",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class CharacterLSTMPreprocessModule(nn.Module):\n",
    "    \"\"\"\n",
    "    CharacterLSTMPreprocessModule takes an `xx` input string, and maps it to an embedding tensor using a\n",
    "    character level LSTM over each input word.  We \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, integerizer):\n",
    "        super().__init__()\n",
    "        \n",
    "        # string integerizer from homework 2\n",
    "        self.integerizer = integerizer\n",
    "        \n",
    "        # the parameters and submodules for this module\n",
    "        self.character_lstm = nn.LSTMCell(len(integerizer), HIDDEN_SIZE)\n",
    "        self.character_lstm_init = nn.Parameter(torch.rand(1, HIDDEN_SIZE))\n",
    "        \n",
    "    def forward(self, *, xx):\n",
    "        \"\"\"\n",
    "        Preprocess the input sequence xx of words into a sequence of word embeddings.\n",
    "        \"\"\"\n",
    "        output = []\n",
    "        # For each word:\n",
    "        for x in xx:\n",
    "            # initalize the hidden state of the LSTM\n",
    "            cx = self.character_lstm_init\n",
    "            hx = torch.tanh(cx)\n",
    "            # Now loop over each character in the word:\n",
    "            for c in x:\n",
    "                # One-hot encoding\n",
    "                i = torch.zeros(1, len(self.integerizer))\n",
    "                i[0, self.integerizer.index(c)] = 1\n",
    "                # Run the LSTM\n",
    "                hx, cx = self.character_lstm(i, (hx, cx))\n",
    "            output.append(hx)\n",
    "        # Stack all gained word embeddings into a single tensor.\n",
    "        return torch.stack(output, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "6fe62c65-ff79-439c-8d39-c2920ea7c710",
    "deletable": false
   },
   "source": [
    "### Simple tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "a29974f2-8138-494e-aa17-2e41efe035f6",
    "scrolled": false,
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Let's test the character LSTM module\n",
    "dutch_characters = set(c for d in iterate_data('train') for c in ' '.join(d.xx))\n",
    "dutch_character_integerizer = Integerizer(tuple(dutch_characters))\n",
    "\n",
    "dutch_preprocess = CharacterLSTMPreprocessModule(dutch_character_integerizer)\n",
    "\n",
    "# Since we inherited from nn.Module, we can now see all parameters automatically:\n",
    "print('Parameters in model:\\n\\t'+ '\\n\\t'.join(\n",
    "    list(name for name, value in dutch_preprocess.named_parameters())))\n",
    "\n",
    "# Get a first example out of the dutch training data and process it:\n",
    "xx, oo, yy = next(iterate_data('train-small'))\n",
    "# This will call the forward() method we defined above\n",
    "xx_processed = dutch_preprocess(xx=xx)\n",
    "\n",
    "# Check that the output is a PyTorch tensor and that the size has\n",
    "# a 1 in the first dimention to represent the mini-batch\n",
    "# the length of the input sentence for the second dimention\n",
    "# the HIDDEN_SIZE to represent the size of the hidden state that we are using to represent words\n",
    "assert xx_processed.shape == (1, len(xx), HIDDEN_SIZE)\n",
    "\n",
    "# Look at the tensors:\n",
    "xx_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "b13bed1d-f907-4bfe-9c50-aceb472fd408",
    "deletable": false
   },
   "source": [
    "You can see that this is a tensor with three dimensions of size 1, 3, and 30 -- and you can already see that PyTorch also stored a function pointer to calculate the gradients. So let's make use of that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "d11702a5-892a-4b88-80fa-ef5aec6919f6",
    "deletable": false
   },
   "source": [
    "### Basic training task for `CharacterLSTMPreprocessModule`\n",
    "\n",
    "The above only checked that the output of `CharacterLSTMPreprocessModule` was the correct shape, however to check that the model is actually capable of *learning* a function, we need to set up a simple task.\n",
    "\n",
    "Below, we have defined `SimpleWordClassification` which takes the embedding of a word and attempts to predict if the word contains the letter 'e'.\n",
    "This model classifies a word by performing $p_i = \\sigma(W e(\\text{word}_i) + b)$, where it applies a linear transform of $Wx + b$ to the output of our embedding layer $e(\\cdot)$ and then a sigmoid function to map to $[0, 1]$.\n",
    "\n",
    "To train this model, we need to define a *training loss*.  For this simple model we define the training loss as the squared difference between the correct label and the predicted label $\\sum_i (l_i - p_i)^2$.  To train the model, we need to take the derivative of *all* the parameters with respect to the scalar that represents the training loss.  This is done for us *automatically* when calling PyTorch's `.backward()` method on the loss tensor.\n",
    "\n",
    "For this simple model, we are choosing to train it using just standard [SGD](http://pytorch.org/docs/master/optim.html#torch.optim.SGD) like in the previous homeworks. However, this function is again provided for by PyTorch.  Using `torch.optim.SGD`, we take a list of all the parameters in the model (which we can get as `model.parameters()`) and a learning rate.  Once we have computed the gradients (as described above), `optimizer.step()` to take a single step in the direction of the gradient.  Note, that the optimizers in PyTorch are always seeking to *minimize* the loss (so if you want to maximize a value do `(-reward).backward()`).\n",
    "\n",
    "If you have correctly implemented `CharacterLSTMPreprocessModule` you should see that large difference between the positive and negative training examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "df418e5f-b7d6-4630-9ead-55bd930c21a7",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class SimpleWordClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embedding = CharacterLSTMPreprocessModule(dutch_character_integerizer)\n",
    "        self.word_classifier = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        \n",
    "    def forward(self, xx):\n",
    "        embedding = self.word_embedding(xx=xx)  # this will call self.word_embedding.forward(....)\n",
    "        return torch.sigmoid(self.word_classifier(embedding))\n",
    "    \n",
    "dutch_words = set(x for d in iterate_data('train') for x in d.xx)\n",
    "# split the words such that there is some classification boundary that we can learn\n",
    "# here we are using words that contain the letter 'e'\n",
    "positive_words = set(word for word in dutch_words if 'e' in word)\n",
    "negative_words = dutch_words - positive_words\n",
    "\n",
    "simple_classifier = SimpleWordClassification()\n",
    "\n",
    "optimizer = torch.optim.SGD(simple_classifier.parameters(), lr=.05)\n",
    "\n",
    "# Let's train for 3 epochs\n",
    "for _ in range(3):\n",
    "    # In each epoch iterate over all words\n",
    "    for word in list(dutch_words):\n",
    "        l_i = int(word in positive_words)\n",
    "        p_i = simple_classifier([word])\n",
    "        \n",
    "        # squared error -- a most simple loss\n",
    "        loss = (l_i - p_i)**2\n",
    "        \n",
    "        # compute the gradient of all parameters with respect to the loss tensor\n",
    "        # This will traverse the computation graph backwards from the `loss` node,\n",
    "        # for each node computing the gradient and storing it in this node's `.grad`.\n",
    "        loss.backward()\n",
    "\n",
    "        # make the optimization method take a single step\n",
    "        # This essentially calls `parameter -= optimizer.lr * parameter.grad`\n",
    "        # for each parameter that it got passed above (simple_classifier.parameters())\n",
    "        # Go ahead and try doing things manually like that yourself!\n",
    "        optimizer.step()\n",
    "\n",
    "        # As the gradients are accumulated into `.grad`, we need to zero it before the\n",
    "        # next update -- or PyTorch will just accumulate the next gradient on top of the old one\n",
    "        #\n",
    "        # (Keeping .step() seperate from .zero_grad() is useful in the case that \n",
    "        # there are multiple operations which will use the same gradients.)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    # quickly test how well the model is doing on the *training* data\n",
    "    # You should know by now that this merely tests if we can *(over)fit*,\n",
    "    # not if we truly \"learn\", that is, generalize to new data!\n",
    "    pos_avg = simple_classifier(list(positive_words)).mean().item()\n",
    "    neg_avg = simple_classifier(list(negative_words)).mean().item()\n",
    "    \n",
    "    print(f\"Avg pos/neg ex prob: {pos_avg:.5f} / {neg_avg:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "fd02fbaf-5c23-433f-9caf-35b57efeeb78",
    "deletable": false
   },
   "source": [
    "Defining the loss as the squared distance $\\sum_i (l_i - p_i)^2$ should have felt silly to you -- why not instead maximize likelihood like we usually do with probabilities?\n",
    "\n",
    "Specifically, let's minimize the *negative log-likelihood* (NLL). What objective is that here? Use the same notation ($l_i$ and $p_i$) as above:\n",
    "\n",
    "$$\\text{minimize}\\; \\color{red}{\\text{FILL IN}}$$\n",
    "\n",
    "Try to replace the squared loss above with the NLL loss! How does the performance compare?\n",
    "\n",
    "$$\\color{red}{\\text{FILL IN}}$$\n",
    "\n",
    "Note that if you run into issues with NaNs, there is most likely an issue in your formula, perhaps a missing sign (this little exercise there is no need to use numerically stable functions like `log1p(x)` instead of `log(1+x)`).\n",
    "But why should you still be a little worried about this code?\n",
    "\n",
    "Even and perhaps especially in neural times we still much prefer to work with log-probabilities throughout. This then also means using functions like `logsigmoid` instead of `sigmoid` and `log_softmax` instead of `softmax`.\n",
    "The entire topic of numerical (in-)stability is very interesting and annoying at the same time, but the guidelines are the same ones that you know: beware of numbers close to 0, especially whenever they interact with numbers that are not.\n",
    "\n",
    "We'll pretend that everything is fine and keep going on with this assignment, though, as later we will naturally end up working with log-probabilities. Still, it will be useful to read a little but about `log1p()`, `expm1()`, and `log_softmax()` as you will likely end up wanting to use them for many calculations when training neural models in a stable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "7a74bd5b-6f5a-4d53-bb5f-c78c5a173e5f",
    "deletable": false
   },
   "source": [
    "## Part 3: Transition-based Tagging\n",
    "\n",
    "We will now learn about a very popular way to perform structured prediction: transition-based models.\n",
    "\n",
    "You will remember that in HW1, we explicitly enumerated all possible outputs $\\mathbf y$ to score them individually and then select the best structure (or sequence) from the set of all $\\mathbf y$'s.\n",
    "In HW2, we circumvented the need for enumeration by using dynamic programming, letting OpenFST do the heavy lifting for us.\n",
    "Since in this homework we want more complicated scoring functions that aren't as nicely decomposable, we will need a new way to build up structures that doesn't require exhaustive enumeration.\n",
    "\n",
    "Transition-based models reduce the search for a structure to a sequence of individual predictions that togther fully describe the structure. For example, let the structure be an IOB tag sequence. Then a good way to decompose the entire sequence into a sequence of \"transitions\" or \"actions\" is to have every one of these actions be the prediction of an individual tag: first predict the first one, then the second one, and so on until we've reached the end of our input.\n",
    "\n",
    "The perhaps less obvious question is: what about state? Transition systems always maintain some state, usually containing a description of things that have been \"assembled\" already (in our case, the sequence of all already predicted tags), but later we will also have this state contain the \"buffer\" of things yet to process. But let's start simple.\n",
    "\n",
    "We will need to define a new kind of `TaskSetting`, one that allows for such a state to be used in proposing new individual actions (i.e., what `iterate_y` does), as depending on the state, not all actions may be available (for example, if our last action was a `O` tag, we cannot propose an `I` tag in the IOB scheme).\n",
    "\n",
    "Below, you can see the base class for these transition tasks.\n",
    "Note that we added a new function `iterate_a` that we will use instead of the old `iterate_y` to help make the distinction clear -- it takes an arbitrary `state`, not just a strictly sequential `yy_prefix`, even though that will be the easiest and most obvious use case.\n",
    "\n",
    "(Maybe `StatefulTaskSetting` should be the standard `TaskSetting` and we should have introduced it as such in Homework 1? You tell us! :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "69526350-82b3-4f37-955a-9b7f2f9df394",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class StatefulTaskSetting(TaskSetting):\n",
    "    def initial_taskstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        The initial state for an input `xx`. Needs to be overriden.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def next_taskstate(self, *, xx, a, taskstate):\n",
    "        \"\"\"\n",
    "        Returns the state that results from the execution of action `a` in state `taskstate`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def iterate_a_s(self, *, xx, oo=None, taskstate):\n",
    "        \"\"\"\n",
    "        This method can iterate all possible actions given a state of the model.\n",
    "        It returns a tuple with both the action and the new state that would result from taking it.\n",
    "        \n",
    "        The default implementation calls the task's iterate_y with state as the yy_prefix\n",
    "        for easier compatibility with our old `TaskSetting`.\n",
    "        \n",
    "        Remember that this method ought to yield `None` (as one possible \"action\")\n",
    "        when the given `taskstate` permits stopping.\n",
    "        \"\"\"\n",
    "        for a in self.iterate_y(xx=xx, oo=oo, yy_prefix=taskstate):\n",
    "            yield (a, self.next_taskstate(xx=xx, a=a, taskstate=taskstate))\n",
    "    \n",
    "    def iterate_a(self, *, xx, oo=None, taskstate):\n",
    "        \"\"\"\n",
    "        Convenience method to only get action proposals without resulting states.\n",
    "        \"\"\"\n",
    "        for a, _ in self.iterate_a_s(xx=xx, oo=oo, taskstate=taskstate):\n",
    "            yield a\n",
    "        \n",
    "    def iterate_aa(self, *, xx, oo=None):\n",
    "        \"\"\"\n",
    "        Returns an iterator over plans that are allowed for input `xx`.\n",
    "        \"\"\"\n",
    "        def _finish_prefix(aa_prefix, taskstate):\n",
    "            \"\"\"Iterate over just those `aa` sequences starting with `aa_prefix`.\"\"\"\n",
    "            for a, s in self.iterate_a_s(xx=xx, oo=oo, taskstate=taskstate):\n",
    "                if a is None:\n",
    "                    yield aa_prefix\n",
    "                else:\n",
    "                    yield from _finish_prefix(aa_prefix + (a,), s)\n",
    "        yield from _finish_prefix(tuple(), self.initial_taskstate(xx=xx))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "7e05b5c7-bf31-4d4d-9045-12ebbf221f8c",
    "deletable": false
   },
   "source": [
    "With this new scaffolding, we can mostly reuse the old `IobTask0` from HW2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "2252d258-299f-4a19-8b9c-762811040464",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class IobStatefulTask(StatefulTaskSetting, IobTask0):\n",
    "    def initial_taskstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        Since for the IOB task we want to iteratively construct the tag\n",
    "        sequence, start with an empty sequence.\n",
    "        \"\"\"\n",
    "        return tuple()\n",
    "\n",
    "    def next_taskstate(self, *, xx, a, taskstate):\n",
    "        \"\"\"\n",
    "        Since for the IOB task our tags are our actions, just add them to the sequence.\n",
    "        \"\"\"\n",
    "        return taskstate + (a,)\n",
    "\n",
    "iob_task = IobStatefulTask()\n",
    "\n",
    "# Convince ourselves that everything still works like before:\n",
    "list(iob_task.iterate_aa(xx=[\"word\", \"word\", \"word\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "83a78f91-51d4-493d-bb6c-1c555c05cffc",
    "deletable": false
   },
   "source": [
    "## IOB Transition Model\n",
    "\n",
    "Now let's talk about the model we want to use. It will have a *backwards LSTM* which is encoding the words we have *yet* to see in the sentence, as well as a forward LSTM which encodes every word that we already saw -- and the IOB tag that we chose to assign to that word!\n",
    "\n",
    "<img width=450px src='images/iob-lstm.png'>\n",
    "\n",
    "In the above figure, we just tagged the third word $x_3$ with tag $y_3$. Each circle represents a step of the LSTM where we are passing the hidden state to the next time step.  As we consume more tokens of the input $\\mathbf x$, we will *pop* off computed vectors of the backwards LSTM (on the right), and we will compute a new representation with the forward LSTM to continue to the next tag.\n",
    "You can look at the [decision agent](#DecisionAgent) below to see exactly how we are going to use the transition model before you attempt to implement it.\n",
    "\n",
    "(Don't confuse this model with a simpler model, in which we would take forward+backward-LSTM approach, but predict all tags in isolation. Our model is strictly more powerful as past tags influence future tags -- but that makes decoding a little bit more challenging.)\n",
    "\n",
    "Since we want to keep the TaskSetting and Model separate and make this neural model fit into the hierarchy we built up in HW1, it is now time to inherit from `ProbabilityModel` (specifically we will inherit from `BoltzmannModel` to show that we can still use the slow brute-force ideas from HW1).\n",
    "\n",
    "At this point we realize that we no will probably want to share a *computational* \"state\" between different sequences (namely those with equal yy_prefixes) -- we therefore introduce a new class that will be made for incremental computations. Its state we will call *model state* to not confuse it with the task state that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "eeb67c44-9db8-4786-9d03-e3a699d662dc",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class IncrementalScoringModel(BoltzmannModel):\n",
    "\n",
    "    def initial_modelstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        Return the initial model state object for this model given `xx`. Default is None.\n",
    "        \"\"\"\n",
    "        return None\n",
    "    \n",
    "    def score_a_s(self, *, xx, a, taskstate, modelstate):\n",
    "        \"\"\"\n",
    "        Returns a tuple with:\n",
    "        1) the score of an action `a` for input example `xx` when the task state\n",
    "             is `taskstate` and the model state in `modelstate` and\n",
    "        2) the new modelstate object if this action was executed.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def score_aa(self, *, xx, aa):\n",
    "        \"\"\"\n",
    "        Will just unroll the entire computation for a single `aa`.\n",
    "        Note that we will not use this function -- it just serves as an illustration\n",
    "        of what's possible using our class design.\n",
    "        \"\"\"\n",
    "        taskstate = self.task.initial_taskstate(xx=xx)\n",
    "        modelstate = self.initial_modelstate(xx=xx)\n",
    "        sum_score = 0\n",
    "        for a in aa:\n",
    "            score, modelstate = self.score_a_s(xx=xx, a=a, taskstate=taskstate, modelstate=modelstate)\n",
    "            taskstate = self.task.next_taskstate(xx=xx, a=a, taskstate=taskstate)\n",
    "            sum_score += score\n",
    "        return sum_score\n",
    "    \n",
    "    def score(self, *, xx, yy):\n",
    "        \"\"\"\n",
    "        For compatibility with the BoltzmannModel methods, define this as a wrapper\n",
    "        around score_aa that pretends aa == yy. We will not use this method for our tasks,\n",
    "        but it serves as a neat visualization that the brute-force techniques from the\n",
    "        HW1 Boltzmann model still work.\n",
    "        \"\"\"\n",
    "        return self.score_aa(xx=xx, aa=yy).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "58800f06-20cf-4c04-806b-e4dd383059c1",
    "deletable": false
   },
   "source": [
    "Let's now implement our neural IOB tagging model to follow this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "dd15fe76-ee14-4efb-baa6-551d62e0cca5",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class TaggingTransitionModel(IncrementalScoringModel, nn.Module):\n",
    "\n",
    "    def __init__(self, task, preprocessor):\n",
    "        # Always initialize the PyTorch module first, so the registration hooks work!\n",
    "        nn.Module.__init__(self)\n",
    "        super().__init__(task)\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        This method from our `ProbabilityModel` now initializes neural nets.\n",
    "        \"\"\"\n",
    "        # The LSTMs that encode the input sequence.\n",
    "        # The input for the forward state should be the word embedding of x_i\n",
    "        # and, one-hot encoded, the tag y_i we are considering for it.\n",
    "        self.forward_lstm = nn.LSTMCell(HIDDEN_SIZE + len(self.task.Y_alphabet), HIDDEN_SIZE)\n",
    "        self.backward_lstm = nn.LSTMCell(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "        # Score the current proposal given the hidden forward and backward LSTMs state.\n",
    "        self.score_function = nn.Linear(HIDDEN_SIZE*2, 1)\n",
    "        \n",
    "        # The initial hidden states, represents the EOS and BOS for the respective LSTMs\n",
    "        self.forward_lstm_init = nn.Parameter(torch.rand(1, HIDDEN_SIZE))\n",
    "        self.backward_lstm_init = nn.Parameter(torch.rand(1, HIDDEN_SIZE))\n",
    "\n",
    "    def initial_modelstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        Return the initial modelstate object for this model: the word embeddings,\n",
    "        the lookahead (i.e., backwards) LSTM states, and the initial state for the\n",
    "        forward LSTM.\n",
    "        \"\"\"\n",
    "        # Run the preprocessor to generate a representation for each token in the input xx.\n",
    "        # We will pop the elements off this list as we read and use them during prediction.\n",
    "        xx_embedding = self.preprocessor(xx=xx)\n",
    "        \n",
    "        # Initialize the backward LSTM\n",
    "        cx = self.backward_lstm_init\n",
    "        hx = torch.tanh(cx)\n",
    "        lookahead = [hx]\n",
    "        \n",
    "        # Run backwards through the word of the sentence\n",
    "        for i in range(xx_embedding.shape[1]-1, -1, -1):\n",
    "            # Compute next representation and save it\n",
    "            hx, cx = self.backward_lstm(xx_embedding[:,i], (hx, cx))\n",
    "            lookahead.append(hx)\n",
    "            \n",
    "        # Initialize the forward LSTM\n",
    "        cx = self.forward_lstm_init\n",
    "        hx = torch.tanh(cx)\n",
    "        \n",
    "        return xx_embedding, tuple(lookahead), (hx, cx)\n",
    "    \n",
    "    def score_a_s(self, *, xx, a, taskstate, modelstate):\n",
    "        \"\"\"\n",
    "        Returns the unnormalized log-probability of an action `a` for input example `xx`\n",
    "        when the task state is in `state` and the model state in `modelstate`.\n",
    "        \n",
    "        Note that if we were to follow PyTorch conventions, we would likely call this\n",
    "        method `forward()`, so it would be executed whenever we \"call the object\".\n",
    "        But we'll prefer to keep the names meaningful here.\n",
    "        \"\"\"\n",
    "        # Easy access to the modelstate object\n",
    "        xx_embedding, lookahead, (hx, cx) = modelstate\n",
    "        \n",
    "        # Get the representations for this timestep\n",
    "        # Since we remove all previously used x tokens off the list xx_embedding,\n",
    "        # the first element is the one we are interesting in now.\n",
    "        x = xx_embedding[:, 0]  \n",
    "        yi = torch.zeros(1, 3)\n",
    "        yi[0, self.task.Y_alphabet.index(a)] = 1\n",
    "        \n",
    "        # Compute the new hidden states (hx, cx) of the forward LSTM and the score function\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n",
    "        \n",
    "        # Return score and the next modelstate\n",
    "        assert score.dim() == 0, f\"Score shape is {score.shape}, but should be {torch.tensor(0.0).shape}\"\n",
    "        return score, (xx_embedding, lookahead, (hx, cx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "c6cad269-3d4c-4e99-9f06-5924a70e47fe",
    "deletable": false
   },
   "source": [
    "If you implented it correctly, the score should be a 0-dimensional tensor (i.e., just a scalar).\n",
    "\n",
    "Let us for a moment pretend that we wanted to build a simple locally normalized model. Then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "557661ad-60cb-4390-a79f-c9f6748e6f1a",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "iobtask = IobStatefulTask()\n",
    "model = TaggingTransitionModel(iobtask, dutch_preprocess)\n",
    "\n",
    "print(\"Assuming a global normalization scheme (i.e., use `BoltzmannModel`)\")\n",
    "Z = 0.0\n",
    "for aa in iobtask.iterate_aa(xx=[\"word\", \"word\"]):\n",
    "    p = model.prob(xx=[\"word\", \"word\"], yy=aa)\n",
    "    Z += p\n",
    "    print(aa, f\"{p:.2f}\")\n",
    "print(f\"Z = {Z.item():.3f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Assuming a local normalization scheme\")\n",
    "Z = 0.0\n",
    "for aa in iobtask.iterate_aa(xx=[\"word\", \"word\"]):\n",
    "    xx=[\"word\", \"word\"]\n",
    "    taskstate = model.task.initial_taskstate(xx=xx)\n",
    "    modelstate = model.initial_modelstate(xx=xx)\n",
    "    log_p = 0\n",
    "    if aa[-1] is None:\n",
    "        aa = aa[:-1]\n",
    "    for a_wanted in aa:\n",
    "        scores = []\n",
    "        for a_proposal, state_proposal in model.task.iterate_a_s(xx=xx, taskstate=taskstate):\n",
    "            scores.append(model.score_a_s(xx=xx, a=a_proposal, taskstate=taskstate, modelstate=modelstate)[0])\n",
    "        logZ = torch.logsumexp(torch.stack(scores), dim=-1)\n",
    "        score_wanted, modelstate = model.score_a_s(xx=xx, a=a_wanted, taskstate=taskstate, modelstate=modelstate)\n",
    "        taskstate = model.task.next_taskstate(xx=xx, a=a_wanted, taskstate=taskstate)\n",
    "        log_p += score_wanted - logZ\n",
    "    Z += log_p.exp()\n",
    "    print(aa, f\"{log_p.exp().item():.2f}\")\n",
    "print(f\"Z = {Z.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "c0e97656-f4e2-4e4c-8577-3b8207ccc191",
    "deletable": false
   },
   "source": [
    "As expected, the probabilities of all these different paths seem to sum to one for both normalization schemes... and since we didn't start training yet, the roughly uniform results should not be so surprising... but wait: why are the first two more likely than the last two -- and why *only in the locally normalized model*?\n",
    "\n",
    "$$\\color{red}{\\text{FILL IN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "a3a18628-55f7-466a-b81d-0c4f5a77c95f",
    "deletable": false
   },
   "source": [
    "### Training a Transition Model\n",
    "\n",
    "To train our transition model, we would ideally like to maximize the log probability of $P(\\mathbf y \\mid \\mathbf x)$, however, we are unable to efficiently compute the normalizing function $Z(\\mathbf x)$ as we do not have a dynamic program over the $\\mathbf y$ strings and thus would require computing the exponentially sized $\\mathcal{Y}$ set. (What we did earlier, where we called the BoltzmannModel methods is very much not feasible for bigger inputs -- try it!)\n",
    "\n",
    "Instead, our first attempt will be to try to train our transition model *to assign a high score to the correct path* when using a decision agent.\n",
    "\n",
    "Below, you will find the appropriately named `BrokenGreedyDecisionAgent`.  This agent correctly implements the *interface* for working with a `IncrementalScoringModel`, however when trying to use it for training you will observe some strange behavior (that we will correct later in this assignment).\n",
    "\n",
    "(Note that here we decided to stick to the PyTorch convention of having \"the main computation\" in the `forward()` method.)\n",
    "<a name=\"DecisionAgent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "fa47aca5-9b9c-4523-81aa-2fd180979c1f",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class PyTorchDecisionAgent(nn.Module, DecisionAgent):\n",
    "    \"\"\"\n",
    "    Base class for our Decision Agent class using the PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        # init the base pytorch module\n",
    "        nn.Module.__init__(self)\n",
    "        self.model = model\n",
    "\n",
    "    @property\n",
    "    def task(self):\n",
    "        return self.model.task\n",
    "        \n",
    "    def decision(self, *, xx, oo=None):\n",
    "        \"\"\"\n",
    "        Call the self.forward() method which implements the decision rule\n",
    "        \"\"\"\n",
    "        assert oo is None, \"Our Neural models do not support oo values\"\n",
    "        loss, best_aa = self(xx=xx)  # through pytorch this calls self.forward()\n",
    "        return best_aa\n",
    "\n",
    "    def forward(self, *, xx, aa=None):\n",
    "        \"\"\"\n",
    "        Implement the decision agent's decoding rule.\n",
    "        \n",
    "        Arguments:\n",
    "            `xx`: The input string\n",
    "            `aa`: The our desired output.  If unset, run the decoding rule and select the \"best\" aa\n",
    "        \n",
    "        Returns a tuple of (loss of the selected aa, and the task and model state (in a tuple) that aa led to)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "ce897083-b96e-4345-b385-d17c050481bf",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class BrokenGreedyDecisionAgent(PyTorchDecisionAgent):\n",
    "    \"\"\"\n",
    "    This Decision Agent performs a greedy decoding of output sequence.\n",
    "    It is complete and ment to serve as an example for how you can structure other decision agents in this\n",
    "    homework, however it is *broken* as we will see later in this assignment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, *, xx, aa=None):\n",
    "        # create the initial state for our decoding model\n",
    "        taskstate = self.model.task.initial_taskstate(xx=xx)\n",
    "        modelstate = self.model.initial_modelstate(xx=xx)\n",
    "        \n",
    "        loss = 0  # this is the loss that we are collecting at each step\n",
    "        best_aa = []\n",
    "        while True:\n",
    "            actions = []\n",
    "            action_scores = []\n",
    "            action_taskstates = []\n",
    "            action_modelstates = []\n",
    "            \n",
    "            # Calculate all actions' consequences -- if `yy` is passed we don't\n",
    "            # technically need this, but for simplicity we calculate it anyway.\n",
    "            for a, ntaskstate in self.model.task.iterate_a_s(xx=xx, taskstate=taskstate):\n",
    "                if a is None:\n",
    "                    continue\n",
    "                # call the forward method on the model\n",
    "                # The model should return a score and a new state\n",
    "                nscore, nmodelstate = self.model.score_a_s(xx=xx, a=a, taskstate=taskstate, modelstate=modelstate)\n",
    "                actions.append(a)\n",
    "                action_scores.append(nscore)\n",
    "                action_taskstates.append(ntaskstate)\n",
    "                action_modelstates.append(nmodelstate)\n",
    "\n",
    "            if len(actions) == 0:\n",
    "                # there were no new actions to perform, so we must have reached the end\n",
    "                break\n",
    "\n",
    "            if aa is not None:\n",
    "                # then there is a gold yy sequence, so we are going to \"choose\" the action that matches the gold path\n",
    "                action = actions.index(aa[0])\n",
    "                aa = aa[1:]\n",
    "            else:\n",
    "                # there is no gold action, so choose the one that is the \"best\"\n",
    "                action = max(range(len(actions)), key=lambda i: action_scores[i].item())\n",
    "            \n",
    "            # we are trying to minimize the loss, so we add the negative action score\n",
    "            # as we want to maximize the action score of along this path\n",
    "            loss += -action_scores[action]\n",
    "            best_aa.append(actions[action])\n",
    "            \n",
    "            # the state now holds whatever state was generated by our underlying model of the selected action\n",
    "            taskstate = action_taskstates[action]\n",
    "            modelstate = action_modelstates[action]\n",
    "        # we are done decoding this sequence\n",
    "        # return the loss that we calculated along the way as well as the action sequence that was decided to be best\n",
    "        return loss, tuple(best_aa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "ca10b9b7-198f-4464-a9d0-7c6d958b6f20",
    "deletable": false
   },
   "source": [
    "The `BrokenGreedyDecisionAgent` correctly implements the *interface* for working with our models, so you can use this to tests that you have correctly implemented `CharacterLSTMPreprocessModule` and `IobModule`, however when training you will observe some strange behavior.\n",
    "\n",
    "But before we again need to define a \"Trainer\" like the `SGDTrainer` in HW1.\n",
    "We will be a bit more PyTorch-specific here, as you will see, and we will not use \"vanilla\" SGD, but [Adam](http://pytorch.org/docs/master/optim.html#torch.optim.Adam), an accelerated descent method.\n",
    "For convenience, this model will print out a running (averaged) loss while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "0a3ddfc4-ce10-4046-bbf4-6168b31ca009",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import time\n",
    "class PyTorchTrainer(object):\n",
    "    \n",
    "    def __init__(self, trainable_agent, epochs=1, evaluate=None, optimizer=None):\n",
    "        self._model = trainable_agent\n",
    "        if optimizer is None:\n",
    "            # create an optimizer with the default settings\n",
    "            # model.parameters() is a list of all the trainable parameters in the model\n",
    "            optimizer = torch.optim.Adam(self._model.parameters()) #, lr=0.05)\n",
    "        self._optimizer = optimizer\n",
    "        self._epochs = epochs\n",
    "        self._evaluate = evaluate\n",
    "        \n",
    "    def train(self, dataset):\n",
    "        iteration = 0\n",
    "        dataset = list(dataset)\n",
    "        running_loss = 0\n",
    "        start = time.time()\n",
    "        for _ in range(self._epochs):\n",
    "            shuffle(dataset)\n",
    "            for example in dataset:\n",
    "                # compute the loss from the model (trainable decision agent)\n",
    "                loss, _ = self._model(xx=example.xx, aa=example.yy)\n",
    "                # compute the gradients on the computation graph\n",
    "                loss.backward()  \n",
    "                # make the optimizer take a step using the computed gradients\n",
    "                self._optimizer.step()\n",
    "                # zero out the gradients for the next step\n",
    "                self._optimizer.zero_grad()\n",
    "                running_loss = running_loss * .99 + float(loss)\n",
    "                iteration += 1\n",
    "                if iteration % 50 == 0:\n",
    "                    # print progress\n",
    "                    done = iteration / (len(dataset) * self._epochs)\n",
    "                    now = time.time()\n",
    "                    total_time = (now - start) / done\n",
    "                    sys.stdout.write(f'\\r\\ttrained for {iteration} iterations, loss {running_loss/100} '\n",
    "                                     f'{int(done*100)}% time: {int((now - start) / 60)}/{int(total_time / 60)} (min)  ') \n",
    "            if self._evaluate:\n",
    "                sys.stdout.write('\\n')\n",
    "                self._evaluate(self._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "099e2b79-157a-4318-92aa-e88c7803cf56",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "iob_model = TaggingTransitionModel(IobStatefulTask(),\n",
    "                                   CharacterLSTMPreprocessModule(dutch_character_integerizer))\n",
    "\n",
    "greedy_iob = BrokenGreedyDecisionAgent(iob_model)\n",
    "\n",
    "trainer = PyTorchTrainer(\n",
    "    greedy_iob, \n",
    "    epochs=3,\n",
    "    evaluate=lambda x: x.test_F1(iterate_data('dev'))\n",
    ")\n",
    "\n",
    "%time trainer.train(iterate_data('train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "49687e74-65a2-4e0e-b978-068c065bc787",
    "deletable": false
   },
   "source": [
    "1. What loss did your `IobTransitionModel` get after training with the `BrokenGreedyDecisionAgent`?\n",
    "\n",
    "**Answer:** <span style='color:red'>FILL IN</span>\n",
    "\n",
    "2. What is the *lower bound* on the training loss when using `BrokenGreedyDecisionAgent`?\n",
    "\n",
    "**Answer:** <span style='color:red'>FILL IN</span>\n",
    "\n",
    "3. How well does your model perform on the development data after using `BrokenGreedyDecisionAgent`?\n",
    "\n",
    "**Answer:** <span style='color:red'>FILL IN</span>\n",
    "\n",
    "4. Just what is going wrong here? How is the agent broken?\n",
    "\n",
    "**Answer:** <span style='color:red'>FILL IN</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "7a13a093-ffd7-42e1-8c51-f7973091c7f2",
    "deletable": false
   },
   "source": [
    "### Locally Normalized Greedy Decision Agent\n",
    "\n",
    "\n",
    "In the above decision agent we defined the training loss to be the sum of score assigned to each action ($-\\sum_i G_{y_i}(\\mathbf s_i)$).  Now we are going to try changing this to a locally normalized model or a [MEMM](https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model).\n",
    "\n",
    "<img width=450px src='images/memm.png'>\n",
    "\n",
    "In an MEMM, we are going to define the training loss to be the *negative log probability* (remember our discussion of NLL above? Now is the time to put all that to use!) of all the decisions that we made along the way.\n",
    "As unnormalized log-probability we will use the score that our model gives us.\n",
    "\n",
    "Thus our training loss is $-\\log \\prod_i p(\\mathbf s_i) = - \\sum_i \\log p(\\mathbf s_i)$.\n",
    "\n",
    "Your task will now be to translate this into PyTorch, using the skeleton of the broken agent. Remember, to try to use numerically stable functions whenever you are dealing with probabilities and logarithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "2799c01e-25b5-45c5-a775-70bf0e74bdce",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class LocallyNormalizedGreedyDecisionAgent(PyTorchDecisionAgent):\n",
    "    \n",
    "    def forward(self, *, xx, aa=None):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n",
    "        \n",
    "        # return the loss that we calculated along the way as well as the final state\n",
    "        # From it we can later read off yy (for our IOB task it *is* the tag sequence yy)!\n",
    "        return loss, tuple(best_aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "d08a207d-1be5-45f0-8585-f4cac0cbbfcb",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "iob_model = TaggingTransitionModel(IobStatefulTask(),\n",
    "                                   CharacterLSTMPreprocessModule(dutch_character_integerizer))\n",
    "\n",
    "greedy_iob = LocallyNormalizedGreedyDecisionAgent(iob_model)\n",
    "\n",
    "trainer = PyTorchTrainer(\n",
    "    greedy_iob, \n",
    "    epochs=3,\n",
    "    evaluate=lambda x: x.test_F1(iterate_data('dev')))\n",
    "\n",
    "%time trainer.train(iterate_data('train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "586e6fe0-b168-4f8e-8bef-1f0414bc5203",
    "deletable": false
   },
   "source": [
    "1. What is the lower bound of the loss for `LocallyNormalizedGreedyDecisionAgent`?\n",
    "\n",
    "**Answer**: <span style='color:red'>FILL IN</span>\n",
    "\n",
    "2. How well does `LocallyNormalizedGreedyDecisionAgent` perform on the Iob task? Is it better or worse than the broken agent?\n",
    "\n",
    "**Answer**: <span style='color:red'>FILL IN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "09f4029c-e3d4-488c-a994-5ad14528c178",
    "deletable": false
   },
   "source": [
    "## Beam decoding\n",
    "\n",
    "In the greedy decoding agent above, we are only looking at the one best action.  Additionally, we are only looking at the actions that we can take given that we *made* to to the correct action so far, and as such we are only able to perform local normalization.\n",
    "\n",
    "Instead, we would like to work with a *globally* normalized model (like we have seen in the previous two assignments).  This requires that we are able to compute a $Z(\\mathbf x)$.  However, given that we are unable to efficiently compute the exact value of $Z(\\mathbf x)$, we are going to settle for a *lower bound* to $Z$.\n",
    "\n",
    "For that, let's build a more powerful decision agent: we are going to use [Beam search](https://en.wikibooks.org/wiki/Artificial_Intelligence/Search/Heuristic_search/Beam_search).  Instead of just greedily maintaining the one-best hypothesis, we are going to have a list of the top $k$ possibilities.\n",
    "\n",
    "You will surely have seen beam search as a way to perform inference *at test time*, but we can also lower-bound the $Z(x)$ in a globally normalized model by $\\sum_{j=0}^k \\exp(G(\\mathbf s_n^{(i)}))$, the sum of scores of sequences that made it to the end of the beam. If the ones that didn't make it there had very low probability to begin with, this lower bound will come close enough to $Z(x)$. This is of course a very naive way to train (and it will not work well for us here), but before discussing that, let's see how far it gets us.\n",
    "\n",
    "Here is the basic algorithmic description of beam search:\n",
    "\n",
    "```python\n",
    "beam = [ (0, initial_state) ]\n",
    "while not done:\n",
    "    next_beam = []\n",
    "    for item on beam:\n",
    "        for next possible y actions for item:\n",
    "            g, next_state = f(item, y) \n",
    "            score = ???\n",
    "            next_beam.add((score, next_state))\n",
    "    beam = choose top k items from next_beam\n",
    "loss = -log P(gold sequence) = -log exp(gold) / Z(x)\n",
    "```\n",
    "\n",
    "Hints: \n",
    "1. This is a globally normalized model, the score of a sequence is the *sum* of scores from all steps -- instead of the score of just the most recent operation.\n",
    "2. When you are training your model, it is possible that the *gold* sequence falls off of the beam and will not make it to the end. That's bad and we should avoid it -- hard code the gold sequence, if we have it, to always stay in the beam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "ba4d575b-b86d-489c-88bf-80e605b3a414",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class BeamDecisionAgent(PyTorchDecisionAgent):\n",
    "        \n",
    "    def __init__(self, *args, beam_size=10, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.beam_size = beam_size\n",
    "    \n",
    "    def forward(self, *, xx, aa=None):\n",
    "        \"\"\"\n",
    "        Perform beam search\n",
    "        return the loss for the best path and the best action sequence `aa`\n",
    "            xx is the input sequence\n",
    "            aa (if set) represents the gold sequence.  Return the loss for the aa sequence\n",
    "        \"\"\"\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n",
    "        \n",
    "        if aa:\n",
    "            # if there is a gold yy sequence, then we should return that path\n",
    "            # loss should correspond to the path that we are returning\n",
    "            assert tuple(best_aa) == tuple(aa), f\"Fell off: {best_aa} != {aa}\"\n",
    "        \n",
    "        return loss, best_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "fa919a90-a354-455e-aec2-7acce43455ff",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "iob_model = TaggingTransitionModel(IobStatefulTask(),\n",
    "                                   CharacterLSTMPreprocessModule(dutch_character_integerizer))\n",
    "beam_iob = BeamDecisionAgent(iob_model)\n",
    "\n",
    "trainer = PyTorchTrainer(beam_iob, \n",
    "                         epochs=5, \n",
    "                         evaluate=lambda x: x.test_F1(iterate_data('dev')))\n",
    "\n",
    "%time trainer.train(iterate_data('train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "c9c9d2fa-7494-4143-9941-2a3b5abfdbc9",
    "deletable": false
   },
   "source": [
    "That's not a particularly impressive score.  One problem is that we have very little training data.  Furthermore, the beam search model may lose to greedy search when trained in this naive way.  The reason is that the training only ensures that the correct tag sequence has a high score, which is not enough for it to win.  In order to win, all of its prefixes also have to have a high enough score to stay in the beam, so a good training method should take that into account (\"search-aware training\").  This problem was pointed out by [Kulesza and Pereira (2008)](http://papers.nips.cc/paper/3162-structured-learning-with-approximate-inference.pdf).  The obvious solution is to also include updates on prefixes of the beam ([Collins and Roark (2004)](https://aclweb.org/anthology/P04-1015); [Huang et al. (2012)](https://www.aclweb.org/anthology/N12-1015)).  Nonetheless, we will continue with our naive training method for the rest of this assignment. \n",
    "\n",
    "Global normalization is a good modeling idea and beam search is a good search idea.  When combined with better training procedures, they can be very effective ([Wiseman and Rush (2016)](https://arxiv.org/pdf/1606.02960.pdf); [Zhou et al. (2015)](https://www.aclweb.org/anthology/P15-1117); [Andor et al. (2016)](https://arxiv.org/pdf/1603.06042.pdf))."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "b1b2ce66-1ee6-434d-95e2-5e4f779e4e09",
    "deletable": false
   },
   "source": [
    "# Part 4: Recurrent Neural Shift-Reduce Parsing\n",
    "\n",
    "For our next act, we will introduce a recurrent neural shift-reduce\n",
    "parser. In contrast to CKY and Earley's algorithm -- which have a worst-case\n",
    "runtime of $O(n^3)$ for sentences of length $n$ -- shift-reduce parsers\n",
    "run in $O(n)$ time. This gives them practical relevance in\n",
    "real-world applications that benefit from syntactic information. Also, in\n",
    "contrast to 601.465, we will focus on *dependency* parsing, rather than\n",
    "*constituency* parsing.\n",
    "\n",
    "An excellent introduction to dependency parsing may be found in [this book](https://www.amazon.com/Dependency-Synthesis-Lectures-Language-Technologies/dp/1598295969/ref=sr_1_1?ie=UTF8&qid=1524716345&sr=8-1&keywords=dependency+parsing), which may be downloaded for free on\n",
    "the Johns Hopkins network. For completeness, we will briefly overview\n",
    "the formalism. Consider the sentence:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Formally, let a sentence $\\mathbf{x}\n",
    "\\in \\Sigma^*$, where $\\Sigma$ is an alphabet, a finite non-empty set,\n",
    "where we have augmented $\\Sigma$ with a distinguished root symbol\n",
    "*ROOT*. Given a sentence of length $|\\mathbf{x}| = n$, a dependency parse of\n",
    "$\\mathbf{x}$ is a directed tree on\n",
    "$(n+1)$ nodes, namely, the words of $\\mathbf{x}$ and a new symbol ROOT, which is (you guessed it) the tree's root.\n",
    "\n",
    "In this assignment, we will focus on *projective* dependency\n",
    "parsing. Visually, a dependency parse is considered projective if,\n",
    "drawing all the arcs above the sentence, it is the case that none of\n",
    "them cross. The sentence above has a projective parse.\n",
    "(The case of nonprojective parsing is interesting, full of weird heuristics and beautiful formalisms, and absolutely orthogonal to what this assignment is about.)\n",
    "<!-- We refer the reader to [McDonald et al. (2005)](http://www.aclweb.org/anthology/H05-1066) for an example of an English sentence with a non-projective parse.\n",
    "For a formal definition of projectivity, see [Nivre (2008)](https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-056-R1-07-027). -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "ff3f30a0-d54f-4836-af14-129fb8633c87",
    "deletable": false
   },
   "source": [
    "### Question 4.1\n",
    "\n",
    "Before moving onto transition-based parsing, let's consider dependency parsing using an algorithm you already know.  There is a tight relationship between projective dependency parses and constituency parses. In this question, we ask that you design a dynamic-programming algorithm to parse dependency grammar.\n",
    "\n",
    "We expect you to do this the following way: given an input sentence $\\mathbf{x}$ of length $n$, design a sentence-specific *context-free grammar*, that, when provided to CKY, will yield trees that can trivially (i.e., in linear time) be transformed into dependency parses of $\\mathbf{x}$. What is the runtime (in terms of $n$) of this algorithm?\n",
    "\n",
    "**Answer:** $\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "7dee9a62-0680-4f58-b33f-e147fc0c8b8a",
    "deletable": false
   },
   "source": [
    "### Question 4.2\n",
    "\n",
    "If you're on the right track, you will have found an answer that is $\\omega(n^3)$, i.e., it will take at least cubic time, but may be worse. Can we do better? The algorithm in [Eisner 1996](http://cs.jhu.edu/~jason/papers/eisner.acl96.pdf) provides an elegant $O(n^3)$ solution.\n",
    "Better explanations of the algorithm may be found in [Eisner 1997](https://cs.jhu.edu/~jason/papers/eisner.iwpt97.pdf) or [Eisner 2000](https://cs.jhu.edu/~jason/papers/eisner.iwptbook00.pdf).\n",
    "\n",
    "Map the prose specification of the algorithm into pseudocode in a style similar to that used for CKY in the slides of 601.465. Argue that the runtime of this algorithm is, indeed, $O(n^3)$.\n",
    "\n",
    "**Answer:** $\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "b8229739-1653-407d-a3ec-f867c89f8bc3",
    "deletable": false
   },
   "source": [
    "Now, back to shift-reduce parsers. Shift-reduce parsers for\n",
    "constituency trace their heritage back to fundamental work on\n",
    "compilers in the 1960s; see Aho and Ullman (1972) for a\n",
    "history. However, the modern shift-reduce parser for dependency\n",
    "grammar is due to [Nivre (2003)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.15.6213). Shift-reduce parsers rely on a transition\n",
    "system, which we will take to be a quadruple $S = (C, T, c_s, C_t)$,\n",
    "following [Nivre (2013)](http://stp.lingfil.uu.se/~nivre/master/transition.pdf).\n",
    "\n",
    "* $C$ is a set of *configurations* (these will be our task/model states),\n",
    "* $T$ is a set of transitions, each of which is a (partial) function $t: C \\rightarrow C$,\n",
    "* $c_s$ is an initialization function, mapping a sentence $\\mathbf{x}$ to its initial configuration $c_s(\\mathbf{x})$, \n",
    "* $C_t \\subseteq C$ is a set of terminal configurations.\n",
    "\n",
    "A configuration $c$ for a sentence $\\mathbf{x}$ is a triple $c = \\left(\\sigma,\n",
    "\\beta, N \\right)$, where $\\sigma$ is termed the stack, $\\beta$ the\n",
    "buffer, and $N$ is a set of dependency arcs built so far. Note\n",
    "that dependency arcs are represented as an ordered pair $(i, j)$ that\n",
    "denotes a directed arc from the $i^\\text{th}$ word in $\\mathbf{x}$ towards\n",
    "the $j^\\text{th}$ word in $\\mathbf{x}$. \n",
    "\n",
    "A myriad transition systems abound, with a veritable cottage industry\n",
    "dedicated to the development of novel ones. In this work, we will\n",
    "focus on the so-called *arc-standard* system, which consists\n",
    "of the following definitions:\n",
    "\n",
    "\n",
    "* **Initialization:** $c_s(\\mathbf{x}) = ([0], [1, \\ldots, n]), \\emptyset)$\n",
    "* **Terminal:** $C_t = \\{c \\in C :  c = ([0], [0], N)  \\}$\n",
    "* **Transitions:**\n",
    "  * *Shift*: $(\\sigma, [i \\mid \\beta], N) \\Rightarrow ([\\sigma \\mid i], \\beta, N)$ \n",
    "  * *ReduceLeft:* $([\\sigma \\mid i \\mid j], \\beta, N) \\Rightarrow ([\\sigma \\mid j], \\beta, N \\cup \\{(j, i)\\})$ \n",
    "  * *ReduceRight:* $([\\sigma \\mid i \\mid j], \\beta, N) \\Rightarrow ([\\sigma \\mid i], \\beta, N \\cup \\{(i, j) \\})$\n",
    "  \n",
    "We have represented the transitions using the standard notation found\n",
    "in the dependency-parsing literature. To make it explicit, we will\n",
    "clarify a few points. We write $c \\Rightarrow c'$ to indicate that\n",
    "configuration $c \\in C$ transitions to the configuration $c' \\in C$.\n",
    "We write $[\\sigma | i]$ to indicate a stack whose top element is $i$\n",
    "and whose remaining elements are found in $\\sigma$. Similarly, we write\n",
    "$[i \\mid \\beta]$ to indicate a buffer whose first element is $i$ and the\n",
    "remaining elements are $\\beta$.\n",
    "\n",
    "**Important:** To gain a solid intuition for how the arc-standard transition system works, we ask that you view the animation present in the following [slides](https://web.archive.org/web/20180712194612/http://demo.clab.cs.cmu.edu/fa2015-11711/images/b/b1/TbparsingSmallCorrection.pdf), due to [Miguel Ballesteros](http://miguelballesteros.com/).\n",
    "That should also make it obvious why we chose this peculiar alignment for rendering them.\n",
    "\n",
    "**Caveat:** Most dependency parsers also assign an arc label to the arc. We have omitted this for simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "2ca7a4d0-b2f5-48b2-9b0d-0cfc6e68d6a7",
    "deletable": false
   },
   "source": [
    "You'll see that it is necessary to convert the desired dependency trees into action sequences before we can continue -- and the mapping is not as trivial as it was for the tagging case. Look at the following definition of the `ParsingTask` as a `StatefulTaskSetting`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "4943a52c-495b-4bc2-bc97-f834577b4493",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class ParsingTask(StatefulTaskSetting):\n",
    "    \"\"\"\n",
    "    xx = the sentence\n",
    "    aa = the parse tree (represented as the id of the parent where 0 is root)\n",
    "    yy = the shift/reduce actions that build the parse tree \n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_actions(tree):\n",
    "        \"\"\"\n",
    "        Convert the depedency tree (sequence of parents) to the stack shift/reduce operations\n",
    "        \n",
    "        Returns a tuple representing the operations that convert this into a shift/reduce tree.\n",
    "             0 == shift operation\n",
    "             1 == reduce right\n",
    "             2 == reduce left\n",
    "        \"\"\"\n",
    "        stack = [(0, None)]\n",
    "        i = 0\n",
    "        # the stack actions that we have to perform\n",
    "        operations = []\n",
    "        while True:\n",
    "            if len(stack) >= 2 and stack[-2][0] == stack[-1][1] and stack[-1][0] not in tree[i:]:\n",
    "                # the top element of the stack is the child of the second on the top of the stack\n",
    "                operations.append(2)  # left\n",
    "                del stack[-1]\n",
    "            elif len(stack) >= 2 and stack[-2][1] == stack[-1][0]:\n",
    "                # the second element on the stack is the child of the item just added to the stack\n",
    "                operations.append(1)  # right\n",
    "                del stack[-2]\n",
    "            else:\n",
    "                # then we are going to perform a shift action\n",
    "                if i >= len(tree):\n",
    "                    break\n",
    "                operations.append(0)  # shift\n",
    "                id, parent = i + 1, tree[i]\n",
    "                i += 1\n",
    "                stack.append((id, parent))\n",
    "        assert len(stack) == 1 and stack[0][0] == 0\n",
    "        \n",
    "        # return a list of shift and\n",
    "        return tuple(operations)\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_tree(actions):\n",
    "        \"\"\"\n",
    "        Convert stack operations back to the depedency tree\n",
    "        \"\"\"\n",
    "        l = actions.count(0)  # count the number of words\n",
    "        assert l * 2 == len(actions)  # There should be exactly twice as many operations as words\n",
    "        parents = [-1] * l  # the list of parents\n",
    "\n",
    "        stack = [-1]\n",
    "        i = 0\n",
    "        for y in actions:\n",
    "            if y == 0:\n",
    "                # perform a shift operation by adding to the stack word i\n",
    "                stack.append(i)\n",
    "                i += 1\n",
    "            elif y == 1:\n",
    "                # perform shift right by marking the parent of the second element on the stack\n",
    "                # and removing it from the stack\n",
    "                parents[stack[-2]] = stack[-1] + 1\n",
    "                del stack[-2]\n",
    "            else:\n",
    "                # perform shift left\n",
    "                parents[stack[-1]] = stack[-2] + 1\n",
    "                del stack[-1]\n",
    "\n",
    "        assert len(stack) == 1, \"stack should only have the root element when it is done, otherwise this is invalid\"\n",
    "        \n",
    "        return tuple(parents)\n",
    "    \n",
    "    @staticmethod\n",
    "    def iterate_data(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        Iterate through our parse data, converting the trees into a series of\n",
    "        shift/reduce actions for `yy` that we can train and test on\n",
    "        \"\"\"\n",
    "        for example in iterate_trees(*args, **kwargs):\n",
    "            yield Data_type(\n",
    "                xx=example.xx,\n",
    "                oo=None,  # We are not dealing with partial observations in this homework\n",
    "                yy=ParsingTask.convert_to_actions(example.tree)\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def initial_taskstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        The initial state for an input `xx` is a stack that only contains the ROOT symbol\n",
    "        and a buffer with the entire sentence on it.\n",
    "        \"\"\"\n",
    "        (stacksize, buffersize) = 1, len(xx)\n",
    "        return (stacksize, buffersize)\n",
    "        \n",
    "    def next_taskstate(self, *, xx, a, taskstate):\n",
    "        \"\"\"\n",
    "        Increases/decreases sizes according to action `a` taken.\n",
    "        \"\"\"\n",
    "        (stacksize, buffersize) = taskstate\n",
    "        if a == 0:  #shift\n",
    "            return (stacksize + 1, buffersize - 1)\n",
    "        elif a in (1, 2):  # reduce\n",
    "            return (stacksize - 1, buffersize)\n",
    "        else:\n",
    "            assert a is None\n",
    "            return None\n",
    "\n",
    "    def iterate_a(self, *, xx, oo=None, taskstate):\n",
    "        \"\"\"        \n",
    "        Remember that this method ought to yield `None` (as one possible \"action\")\n",
    "        when the given `taskstate` permits stopping.\n",
    "        \"\"\"\n",
    "        stacksize, buffersize = taskstate\n",
    "        \n",
    "        if buffersize > 0:\n",
    "            # there are still tokens that this can process\n",
    "            # yield a shift operation\n",
    "            yield 0  # shift operation\n",
    "        if stacksize > 2:\n",
    "            # these are the shift operations that we are allowed to perform\n",
    "            # if one of these are valid then both of these will valid\n",
    "            yield 1  # right\n",
    "            yield 2  # left\n",
    "        if stacksize == 2 and buffersize == 0:\n",
    "            # Only ROOT and the \"actual\" root word left on the stack: finish\n",
    "            yield 2\n",
    "\n",
    "    def iterate_a_s(self, *, xx, oo=None, taskstate):\n",
    "        for a in self.iterate_a(xx=xx, taskstate=taskstate):\n",
    "            yield a, self.next_taskstate(xx=xx, a=a, taskstate=taskstate)\n",
    "\n",
    "    def reward(self, *, aa, xx, yy):\n",
    "        \"\"\"\n",
    "        The proxy reward of prediction aa on this sentence if the true chunking is yy.\n",
    "        \"\"\"\n",
    "        tree_gen = self.convert_to_tree(yy=aa)\n",
    "        tree_gold = self.convert_to_tree(yy=yy)\n",
    "        \n",
    "        true_pos = sum(a == b for a,b in zip(tree_gen, tree_gold))\n",
    "        \n",
    "        exact = int(tree_gen == tree_gold)\n",
    "        return true_pos\n",
    "\n",
    "    def reward_F1_triple(self, *, aa, xx, yy):\n",
    "        \"\"\"\n",
    "        returns a triple (true_pos, true, pos) used to compute corpus-level F1:\n",
    "           `true_pos` is the number of \"true positives\" (chunks reported by `aa` that are truly in `yy`)\n",
    "           `true` is the number of chunks that are truly in `yy`  \n",
    "           `pos` is the number of chunks reported by `aa`\n",
    "        \"\"\"\n",
    "        # aa is the predicted output\n",
    "        # yy is the gold label from the dataset\n",
    "        # both have already been converted to the shift/reduce operations\n",
    "        # so we have to convert it back into a tree which lists the parents\n",
    "        tree_gen = self.convert_to_tree(aa)\n",
    "        tree_gold = self.convert_to_tree(yy)\n",
    "        \n",
    "        true_pos = sum(a == b for a,b in zip(tree_gen, tree_gold))\n",
    "        return np.array([len(tree_gold), len(tree_gen), true_pos])\n",
    "\n",
    "    \n",
    "parsing_task = ParsingTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "903e5781-6f65-478c-8234-6dceda6ab4fe",
    "deletable": false
   },
   "source": [
    "Please study the following output carefully, to gain an intuition for why the above code converts\n",
    "dependency parses into sequences of shift and reduce actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "429718de-34e2-4683-9514-6a20666ad816",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print(ParsingTask.convert_to_actions(tree=(0, 1, 1, 1, 7, 7, 1)))\n",
    "print(ParsingTask.convert_to_tree(actions=(0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 1, 2, 2)))\n",
    "\n",
    "for tree in set(p.tree for p in iterate_trees('train')):\n",
    "    actions = ParsingTask.convert_to_actions(tree)\n",
    "    tree2 = ParsingTask.convert_to_tree(actions)\n",
    "    assert tree == tree2\n",
    "    assert len(actions) == len(tree)*2  # there will always be twice as many shift/reduce operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "fc376dc2-7d5d-4f84-9a19-b1a84f3bd483",
    "deletable": false
   },
   "source": [
    "### Question 4.3\n",
    "\n",
    "Why is `ParsingTask` a stateful task? Why is the task state necessary?\n",
    "\n",
    "Answer: $\\color{red}{\\text{FILL IN}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "1a6674e5-5c34-44cb-bae3-cace9960359d",
    "deletable": false
   },
   "source": [
    "## A neural parsing model\n",
    "\n",
    "Now for the final bit of coding: you will implement the `score_a_s` method of the `ParsingModel` we defined below -- you'll see that it is very similar to the IOB model we defined above: we again have a \"lookahead\" LSTM (corresponding to the buffer for our transition-based system), a forward-LSTM of sorts (that now branches though, so that you will have to figure out), and scoring of actions to take in certain task and model states.\n",
    "\n",
    "The network components have been initialized for you in `initialize_params()` -- `*_network_1` and `*_network_2` are to be stacked together to create a deeper network and the rest... you'll figure it out!\n",
    "You may want to look at [Dyer et al. (2015)](https://arxiv.org/abs/1505.08075) to see how to perform the transition system actions with LSTMs; their figures will give you a rough idea of what the entire enterprise should look like (note though, that our model is a lot simpler! We for example omit the LSTM for the action sequence that they render in the lower middle of Figure 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "3c0b54c4-0dc6-461c-a980-8cf6f14b3315",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class ParsingModel(IncrementalScoringModel, nn.Module):\n",
    "    \n",
    "    def __init__(self, task, preprocessor):\n",
    "        # Always initialize the PyTorch module first, so the registration hooks work!\n",
    "        nn.Module.__init__(self)\n",
    "        super().__init__(task)\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def initialize_params(self):\n",
    "        self.backward_lstm = nn.LSTMCell(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        self.forward_lstm = nn.LSTMCell(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "        \n",
    "        # judge two hidden-state-sized vectors and a (one-hot) action\n",
    "        self.action_network_1 = nn.Linear(HIDDEN_SIZE*2+3, HIDDEN_SIZE)\n",
    "        self.action_network_2 = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        \n",
    "        # network for combining two elements on the stack to make a new representation for this tree\n",
    "        self.combine_network_1 = nn.Linear(HIDDEN_SIZE*2, HIDDEN_SIZE*2)\n",
    "        self.combine_network_2 = nn.Linear(HIDDEN_SIZE*2, HIDDEN_SIZE)\n",
    "        \n",
    "        self.lstm_init_backwards = nn.Parameter(torch.rand(1, HIDDEN_SIZE))\n",
    "        self.lstm_init_forwards = nn.Parameter(torch.rand(1, HIDDEN_SIZE))\n",
    "        \n",
    "        self.stack_root = nn.Parameter(torch.rand(1, HIDDEN_SIZE))\n",
    "    \n",
    "    def initial_modelstate(self, *, xx):\n",
    "        \"\"\"\n",
    "        xx is going to be the result of the first encoding pass\n",
    "        and then we want to run the backwards LSTM over this xx sequence\n",
    "        \"\"\"\n",
    "        # Run the preprocessor to generate a representation for each token in the input xx\n",
    "        xx_embedding = self.preprocessor(xx=xx)\n",
    "        \n",
    "        # this will represent the backwards LSTM over the words that are in the sentence\n",
    "        buffer = []\n",
    "        \n",
    "        cx = self.lstm_init_backwards\n",
    "        hx = torch.tanh(cx)\n",
    "        buffer.append(hx)\n",
    "        for i in range(xx_embedding.shape[1]-1,-1,-1):\n",
    "            hx, cx = self.backward_lstm(xx_embedding[:,i], (hx, cx))\n",
    "            buffer.append(hx)\n",
    "        cx = self.lstm_init_forwards\n",
    "        hx = torch.tanh(cx)\n",
    "        # the pre root element on the stack\n",
    "        stack = [\n",
    "            (cx, hx, None),  # the -1 element doesn't need an embedding as this will \n",
    "        ]\n",
    "        \n",
    "        # push the root stack element\n",
    "        hx, cx = self.forward_lstm(self.stack_root, (hx, cx))\n",
    "        stack.append((cx, hx, self.stack_root))\n",
    "        \n",
    "        # return a tuple\n",
    "        return xx_embedding, stack, buffer\n",
    "\n",
    "    def score_a_s(self, *, xx, a, taskstate, modelstate):\n",
    "        ### STUDENTS START\n",
    "        raise NotImplementedError()  # REPLACE ME\n",
    "        ### STUDENTS END\n",
    "        # return score and new modelstate\n",
    "        assert score.dim() == 0, f\"Score {score} has shape {score.shape}, but should be {torch.tensor(0.0).shape}\"\n",
    "        return score, (xx_embedding, stack, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "65763a9c-b63e-471d-b45f-b2a29f9f1842",
    "deletable": false
   },
   "source": [
    "Let's try to train this model with our two different (non-broken) agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "2b6253af-8fec-4f6b-9c9e-968ca09fc6b4",
    "scrolled": true,
    "deletable": false
   },
   "outputs": [],
   "source": [
    "english_character_integerizer = Integerizer(tuple(set(w for d in iterate_trees('train') for w in ' '.join(d.xx))))\n",
    "\n",
    "parsing_model = ParsingModel(parsing_task,\n",
    "                             CharacterLSTMPreprocessModule(english_character_integerizer))\n",
    "\n",
    "greedy_parser = LocallyNormalizedGreedyDecisionAgent(parsing_model)\n",
    "\n",
    "greedy_trainer = PyTorchTrainer(\n",
    "    greedy_parser, \n",
    "    epochs=3, \n",
    "    evaluate=lambda x: x.test_F1(parsing_task.iterate_data('dev')))\n",
    "\n",
    "%time greedy_trainer.train(parsing_task.iterate_data('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "c998debe-42d6-43bc-bb8b-d2248d120287",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "beam_parser = BeamDecisionAgent(parsing_model, beam_size=15)\n",
    "\n",
    "beam_trainer = PyTorchTrainer(\n",
    "    beam_parser,\n",
    "    epochs=3, \n",
    "    evaluate=lambda x: x.test_F1(parsing_task.iterate_data('dev')))\n",
    "    \n",
    "%time beam_trainer.train(islice(parsing_task.iterate_data('train'), 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "22a95910-72c2-44a2-b05c-20636769adba",
    "deletable": false
   },
   "source": [
    "## Speedups not covered in this assignment\n",
    "\n",
    "1. In making a neural model that you actually want to work with, you would probably want to spend some time thinking about how to perform mini-batching.  One of the main reasons that this will speed up our program, is that we are currently performing matrix-vector operations in most cases, in that there is only a single sentence that we are processing at a time.  When mini-batching, you are able to process more than once sentence at a time, this allows for using matrix-matrix operations which will be must faster.  Additionally, less time will be spent on the overhead of PyTorch/Python per sentence we are processing.  Making good use of a GPU will generally require that there are larger matrix-matrix operations which are being performed.\n",
    "\n",
    "2. Given that this network is *dynamically structured* based off the input of the yy sequence, it can sometimes be difficult to identify a good way to mini-batch multiple sentences at the same time.  An alternate approach is to distribute computation between a number of works that are each processing a single sentence at a time.  This allows us to take advantage of the fact that the sentences themselves are independent from each other while still being able to utilize resources.  [Pytorch multiprocessing documentation](http://pytorch.org/docs/master/notes/multiprocessing.html)\n",
    "\n",
    "3. You could also try changing the `num_threads` at the top of this notebook to experiment with the number of threads that are used for [BLAS](http://www.netlib.org/blas/) operations.  BLAS is the library that backs all of the matrix operations that you perform in virtually every other library or language (including R, numpy or matlab).  Assuming that you have *sufficiently large* matrices and have enough CPUs cores on your computer, increasing the number of threads that you are using can improve the runtime.\n",
    "\n",
    "\n",
    "If you want to try to improve the performance of the parser that we built in this notebook, you can try increasing the `HIDDEN_SIZE` that is used throughout.  We set it to 30 by default, but if you set it to larger value like 300, you should expect to see a significant improvement.  Note that the runtime performance scales $O(\\texttt{HIDDEN_SIZE}^2)$ so changing the value to 300 will make it take 100 times longer to train assuming that you don't apply any other techniques to speedup your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "g.cell_uuid": "ffaecb4f-01d2-4503-9f7e-b0a2aa29a55a",
    "deletable": false
   },
   "source": [
    "# The final test !!!\n",
    "\n",
    "Now that you have completed the assignment with the development datasets, try your models on the test dataset for 1 final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "d1b3293c-adeb-4195-8c00-1068bb66c801",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "### STUDENTS START\n",
    "### iob_agent = ???\n",
    "raise NotImplementedError()  # REPLACE ME\n",
    "### STUDENTS END\n",
    "\n",
    "iob_agent.test_F1(iterate_data('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "d2e99359-4662-44d3-921b-81f859517388",
    "deletable": false
   },
   "outputs": [],
   "source": [
    "### STUDENTS START\n",
    "### parse_agent = ???\n",
    "raise NotImplementedError()  # REPLACE ME\n",
    "### STUDENTS END\n",
    "\n",
    "parse_agent.test_F1(parsing_task.iterate_data('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "g.cell_uuid": "15181ca3-9352-40bf-9080-9093c40305bf",
    "deletable": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}